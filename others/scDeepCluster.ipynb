{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set recommended library versions\n",
    "# !pip install tensorflow==1.15.0\n",
    "# !pip install keras==2.1.4\n",
    "# !pip install pandas==1.0.4\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run scDeepCluster on the simulated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This part implements the scDeepCluster algoritm\n",
    "\"\"\"\n",
    "\n",
    "from time import time\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras.layers import Dense, Input, GaussianNoise, Layer, Activation\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "\n",
    "import h5py\n",
    "import scanpy.api as sc\n",
    "from scDeepCluster_layers import ConstantDispersionLayer, SliceLayer, ColWiseMultLayer\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score, silhouette_score, calinski_harabasz_score\n",
    "from scDeepCluster_loss import poisson_loss, NB, ZINB\n",
    "from scDeepCluster_preprocess import read_dataset, normalize\n",
    "import tensorflow as tf\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(2211)\n",
    "# tf.random.set_seed(2211)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2211)\n",
    "\n",
    "MeanAct = lambda x: tf.clip_by_value(K.exp(x), 1e-5, 1e6)\n",
    "DispAct = lambda x: tf.clip_by_value(tf.nn.softplus(x), 1e-4, 1e4)\n",
    "\n",
    "def cluster_acc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate clustering accuracy. Require scikit-learn installed\n",
    "    # Arguments\n",
    "        y: true labels, numpy.array with shape `(n_samples,)`\n",
    "        y_pred: predicted labels, numpy.array with shape `(n_samples,)`\n",
    "    # Return\n",
    "        accuracy, in [0,1]\n",
    "    \"\"\"\n",
    "    y_true = y_true.astype(np.int64)\n",
    "    assert y_pred.size == y_true.size\n",
    "    D = max(y_pred.max(), y_true.max()) + 1\n",
    "    w = np.zeros((D, D), dtype=np.int64)\n",
    "    for i in range(y_pred.size):\n",
    "        w[y_pred[i], y_true[i]] += 1\n",
    "    from sklearn.utils.linear_assignment_ import linear_assignment\n",
    "    ind = linear_assignment(w.max() - w)\n",
    "    return sum([w[i, j] for i, j in ind]) * 1.0 / y_pred.size\n",
    "\n",
    "\n",
    "def autoencoder(dims, noise_sd=0, init='glorot_uniform', act='relu'):\n",
    "    \"\"\"\n",
    "    Fully connected auto-encoder model, symmetric.\n",
    "    Arguments:\n",
    "        dims: list of number of units in each layer of encoder. dims[0] is input dim, dims[-1] is units in hidden layer.\n",
    "            The decoder is symmetric with encoder. So number of layers of the auto-encoder is 2*len(dims)-1\n",
    "        act: activation, not applied to Input, Hidden and Output layers\n",
    "    return:\n",
    "        Model of autoencoder\n",
    "    \"\"\"\n",
    "    n_stacks = len(dims) - 1\n",
    "    # input\n",
    "    sf_layer = Input(shape=(1,), name='size_factors')\n",
    "    x = Input(shape=(dims[0],), name='counts')\n",
    "    h = x\n",
    "    h = GaussianNoise(noise_sd, name='input_noise')(h)\n",
    " \n",
    "    # internal layers in encoder\n",
    "    for i in range(n_stacks-1):\n",
    "        h = Dense(dims[i + 1], kernel_initializer=init, name='encoder_%d' % i)(h)\n",
    "        h = GaussianNoise(noise_sd, name='noise_%d' % i)(h)    # add Gaussian noise\n",
    "        h = Activation(act)(h)\n",
    "    # hidden layer\n",
    "    h = Dense(dims[-1], kernel_initializer=init, name='encoder_hidden')(h)  # hidden layer, features are extracted from here\n",
    "\n",
    "    # internal layers in decoder\n",
    "    for i in range(n_stacks-1, 0, -1):\n",
    "        h = Dense(dims[i], activation=act, kernel_initializer=init, name='decoder_%d' % i)(h)\n",
    "\n",
    "    # output\n",
    " \n",
    "    pi = Dense(dims[0], activation='sigmoid', kernel_initializer=init, name='pi')(h)\n",
    "\n",
    "    disp = Dense(dims[0], activation=DispAct, kernel_initializer=init, name='dispersion')(h)\n",
    "\n",
    "    mean = Dense(dims[0], activation=MeanAct, kernel_initializer=init, name='mean')(h)\n",
    "\n",
    "    output = ColWiseMultLayer(name='output')([mean, sf_layer])\n",
    "    output = SliceLayer(0, name='slice')([output, disp, pi])\n",
    "\n",
    "    return Model(inputs=[x, sf_layer], outputs=output)\n",
    "\n",
    "\n",
    "class ClusteringLayer(Layer):\n",
    "    \"\"\"\n",
    "    Clustering layer converts input sample (feature) to soft label, i.e. a vector that represents the probability of the\n",
    "    sample belonging to each cluster. The probability is calculated with student's t-distribution.\n",
    "    # Example\n",
    "    ```\n",
    "        model.add(ClusteringLayer(n_clusters=10))\n",
    "    ```\n",
    "    # Arguments\n",
    "        n_clusters: number of clusters.\n",
    "        weights: list of Numpy array with shape `(n_clusters, n_features)` witch represents the initial cluster centers.\n",
    "        alpha: parameter in Student's t-distribution. Default to 1.0.\n",
    "    # Input shape\n",
    "        2D tensor with shape: `(n_samples, n_features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(n_samples, n_clusters)`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_clusters, weights=None, alpha=1.0, **kwargs):\n",
    "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
    "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
    "        super(ClusteringLayer, self).__init__(**kwargs)\n",
    "        self.n_clusters = n_clusters\n",
    "        self.alpha = alpha\n",
    "        self.initial_weights = weights\n",
    "        self.input_spec = InputSpec(ndim=2)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 2\n",
    "        input_dim = input_shape[1]\n",
    "        self.input_spec = InputSpec(dtype=K.floatx(), shape=(None, input_dim))\n",
    "        self.clusters = self.add_weight(shape = (self.n_clusters, input_dim), \n",
    "                                        initializer='glorot_uniform',\n",
    "                                        name='clusters')\n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        \"\"\" student t-distribution, as same as used in t-SNE algorithm.\n",
    "                 q_ij = 1/(1+dist(x_i, u_j)^2), then normalize it.\n",
    "        Arguments:\n",
    "            inputs: the variable containing data, shape=(n_samples, n_features)\n",
    "        Return:\n",
    "            q: student's t-distribution, or soft labels for each sample. shape=(n_samples, n_clusters)\n",
    "        \"\"\"\n",
    "        q = 1.0 / (1.0 + (K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=2) / self.alpha))\n",
    "        q **= (self.alpha + 1.0) / 2.0\n",
    "        q = K.transpose(K.transpose(q) / K.sum(q, axis=1))\n",
    "        return q\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape and len(input_shape) == 2\n",
    "        return input_shape[0], self.n_clusters\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'n_clusters': self.n_clusters}\n",
    "        base_config = super(ClusteringLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "\n",
    "class SCDeepCluster(object):\n",
    "    def __init__(self,\n",
    "                 dims,\n",
    "                 n_clusters=10,\n",
    "                 noise_sd=0,\n",
    "                 alpha=1.0,\n",
    "                 ridge=0,\n",
    "                 debug=False):\n",
    "\n",
    "        super(SCDeepCluster, self).__init__()\n",
    "\n",
    "        self.dims = dims\n",
    "        self.input_dim = dims[0]\n",
    "        self.n_stacks = len(self.dims) - 1\n",
    "\n",
    "        self.n_clusters = n_clusters\n",
    "        self.noise_sd = noise_sd\n",
    "        self.alpha = alpha\n",
    "        self.act = 'relu'\n",
    "        self.ridge = ridge\n",
    "        self.debug = debug\n",
    "        self.autoencoder = autoencoder(self.dims, noise_sd=self.noise_sd, act = self.act)\n",
    "        \n",
    "        # prepare clean encode model without Gaussian noise\n",
    "        ae_layers = [l for l in self.autoencoder.layers]\n",
    "        hidden = self.autoencoder.input[0]\n",
    "        for i in range(1, len(ae_layers)):\n",
    "            if \"noise\" in ae_layers[i].name:\n",
    "                next\n",
    "            elif \"dropout\" in ae_layers[i].name:\n",
    "                next\n",
    "            else:\n",
    "                hidden = ae_layers[i](hidden)\n",
    "            if \"encoder_hidden\" in ae_layers[i].name:  # only get encoder layers\n",
    "                break\n",
    "        self.encoder = Model(inputs=self.autoencoder.input, outputs=hidden)\n",
    "\n",
    "        pi = self.autoencoder.get_layer(name='pi').output\n",
    "        disp = self.autoencoder.get_layer(name='dispersion').output\n",
    "        mean = self.autoencoder.get_layer(name='mean').output\n",
    "        zinb = ZINB(pi, theta=disp, ridge_lambda=self.ridge, debug=self.debug)\n",
    "        self.loss = zinb.loss\n",
    "\n",
    "        clustering_layer = ClusteringLayer(self.n_clusters, alpha=self.alpha, name='clustering')(hidden)\n",
    "        self.model = Model(inputs=[self.autoencoder.input[0], self.autoencoder.input[1]],\n",
    "                           outputs=[clustering_layer, self.autoencoder.output])\n",
    "\n",
    "        self.pretrained = False\n",
    "        self.centers = []\n",
    "        self.y_pred = []\n",
    "\n",
    "    def pretrain(self, x, y, batch_size=256, epochs=200, optimizer='adam', ae_file='ae_weights.h5'):\n",
    "        print('...Pretraining autoencoder...')\n",
    "        self.autoencoder.compile(loss=self.loss, optimizer=optimizer)\n",
    "        es = EarlyStopping(monitor=\"loss\", patience=50, verbose=1)\n",
    "        self.autoencoder.fit(x=x, y=y, batch_size=batch_size, epochs=epochs, callbacks=[es], verbose = 0)\n",
    "        self.autoencoder.save_weights(ae_file)\n",
    "        print('Pretrained weights are saved to ./' + str(ae_file))\n",
    "        self.pretrained = True\n",
    "\n",
    "    def load_weights(self, weights_path):  # load weights of scDeepCluster model\n",
    "        self.model.load_weights(weights_path)\n",
    "\n",
    "    def extract_feature(self, x):  # extract features from before clustering layer\n",
    "        return self.encoder.predict(x)\n",
    "\n",
    "    def predict_clusters(self, x):  # predict cluster labels using the output of clustering layer\n",
    "        q, _ = self.model.predict(x, verbose=0)\n",
    "        return q.argmax(1)\n",
    "\n",
    "    @staticmethod\n",
    "    def target_distribution(q):  # target distribution P which enhances the discrimination of soft label Q\n",
    "        weight = q ** 2 / q.sum(0)\n",
    "        return (weight.T / weight.sum(1)).T\n",
    "\n",
    "    def fit(self, x_counts, sf, y, raw_counts, batch_size=256, maxiter=2e4, tol=1e-3, update_interval=140,\n",
    "            ae_weights=None, save_dir='./output/pickle_results/scDeepCluster', loss_weights=[1,1], optimizer='adadelta'):\n",
    "\n",
    "        self.model.compile(loss=['kld', self.loss], loss_weights=loss_weights, optimizer=optimizer)\n",
    "\n",
    "        print('Update interval', update_interval)\n",
    "        save_interval = int(x_counts.shape[0] / batch_size) * 5  # 5 epochs\n",
    "        print('Save interval', save_interval)\n",
    "\n",
    "        # Step 1: pretrain\n",
    "        if not self.pretrained and ae_weights is None:\n",
    "            print('...pretraining autoencoders using default hyper-parameters:')\n",
    "            print('   optimizer=\\'adam\\';   epochs=200')\n",
    "            self.pretrain(x, batch_size)\n",
    "            self.pretrained = True\n",
    "        elif ae_weights is not None:\n",
    "            self.autoencoder.load_weights(ae_weights)\n",
    "            print('ae_weights is loaded successfully.')\n",
    "\n",
    "        # Step 2: initialize cluster centers using k-means\n",
    "        print('Initializing cluster centers with k-means.')\n",
    "        kmeans = KMeans(n_clusters=self.n_clusters, n_init=20)\n",
    "        self.y_pred = kmeans.fit_predict(self.encoder.predict([x_counts, sf]))\n",
    "        y_pred_last = np.copy(self.y_pred)\n",
    "        self.model.get_layer(name='clustering').set_weights([kmeans.cluster_centers_])\n",
    "\n",
    "        # Step 3: deep clustering\n",
    "        # logging file\n",
    "        import csv, os\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        logfile = open(save_dir + '/scDeepCluster_log.csv', 'w')\n",
    "        logwriter = csv.DictWriter(logfile, fieldnames=['iter', 'acc', 'nmi', 'ari', 'L', 'Lc', 'Lr'])\n",
    "        logwriter.writeheader()\n",
    "\n",
    "        loss = [0, 0, 0]\n",
    "        index = 0\n",
    "        for ite in range(int(maxiter)):\n",
    "            if ite % update_interval == 0:\n",
    "                q, _ = self.model.predict([x_counts, sf], verbose=0)\n",
    "                self.features = self.extract_feature([x_counts, sf])\n",
    "                p = self.target_distribution(q)  # update the auxiliary target distribution p\n",
    "\n",
    "                # evaluate the clustering performance\n",
    "                self.y_pred = q.argmax(1)\n",
    "                if y is not None:\n",
    "#                     acc = np.round(cluster_acc(y, self.y_pred), 5)\n",
    "                    nmi = np.round(metrics.normalized_mutual_info_score(y, self.y_pred), 5)\n",
    "                    ari = np.round(metrics.adjusted_rand_score(y, self.y_pred), 5)\n",
    "                    loss = np.round(loss, 5)\n",
    "                    \n",
    "                # check stop criterion\n",
    "                delta_label = np.sum(self.y_pred != y_pred_last).astype(np.float32) / self.y_pred.shape[0]\n",
    "                y_pred_last = np.copy(self.y_pred)\n",
    "                if ite > 0 and delta_label < tol:\n",
    "                    print('delta_label ', delta_label, '< tol ', tol)\n",
    "                    print('Reached tolerance threshold. Stopping training.')\n",
    "                    logfile.close()\n",
    "                    break\n",
    "\n",
    "            # train on batch\n",
    "            if (index + 1) * batch_size > x_counts.shape[0]:\n",
    "                loss = self.model.train_on_batch(x=[x_counts[index * batch_size::], sf[index * batch_size:]],\n",
    "                                                 y=[p[index * batch_size::], raw_counts[index * batch_size::]])\n",
    "                index = 0\n",
    "            else:\n",
    "                loss = self.model.train_on_batch(x=[x_counts[index * batch_size:(index + 1) * batch_size], \n",
    "                                                    sf[index * batch_size:(index + 1) * batch_size]],\n",
    "                                                 y=[p[index * batch_size:(index + 1) * batch_size],\n",
    "                                                    raw_counts[index * batch_size:(index + 1) * batch_size]])\n",
    "                index += 1\n",
    "\n",
    "            # save intermediate model\n",
    "#             if ite % save_interval == 0:\n",
    "#                 # save scDeepCluster model checkpoints\n",
    "#                 print('saving model to: ' + save_dir + '/scDeepCluster_model_' + str(ite) + '.h5')\n",
    "#                 self.model.save_weights(save_dir + '/scDeepCluster_model_' + str(ite) + '.h5')\n",
    "\n",
    "            ite += 1\n",
    "\n",
    "        # save the trained model\n",
    "        logfile.close()\n",
    "        print('saving model to: ' + save_dir + '/scDeepCluster_model_final.h5')\n",
    "        self.model.save_weights(save_dir + '/scDeepCluster_model_final.h5')\n",
    "        \n",
    "        return self.y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import glob2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for category in [\"real_data\",\n",
    "#                  \"balanced_data\", \"imbalanced_data\"\n",
    "                ]:\n",
    "\n",
    "    path = \"..\"\n",
    "    if category in [\"balanced_data\", \"imbalanced_data\"]:\n",
    "        files = glob2.glob(f'{path}/R/simulated_data/{category}/*.h5')\n",
    "        files = [\n",
    "            f[len(f\"{path}/R/simulated_data/{category}/\"):-3] for f in files\n",
    "        ]\n",
    "    else:\n",
    "        files = glob2.glob(f'{path}/real_data/*.h5')\n",
    "        files = [f[len(f\"{path}/real_data/\"):-3] for f in files]\n",
    "        files = [\n",
    "            'worm_neuron_cell',\n",
    "            'mouse_ES_cell',\n",
    "            'mouse_bladder_cell',\n",
    "            '10X_PBMC',\n",
    "            'Quake_10x_Bladder',\n",
    "            'Young',\n",
    "            'Quake_10x_Limb_Muscle',\n",
    "            'Adam',\n",
    "            'Quake_Smart-seq2_Trachea',\n",
    "            'Quake_Smart-seq2_Limb_Muscle',\n",
    "            'Quake_Smart-seq2_Lung',\n",
    "            'Romanov',\n",
    "            'Muraro',\n",
    "            'Quake_Smart-seq2_Diaphragm',\n",
    "            'Quake_10x_Spleen',\n",
    "        ]\n",
    "    print(files)\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        columns=[\"dataset\", \"ARI\", \"NMI\", \"sil\", \"run\", \"time\", \"pred\", \"cal\", \"features\"])\n",
    "#     df = pd.read_pickle(\n",
    "#                 f\"../output/pickle_results/{category}/{category}_scDeepCluster.pkl\"\n",
    "#             )\n",
    "    print(df.shape)\n",
    "    for dataset in files:\n",
    "        if category in [\"balanced_data\", \"imbalanced_data\"]:\n",
    "            data_mat = h5py.File(\n",
    "                f\"{path}/R/simulated_data/{category}/{dataset}.h5\", \"r\")\n",
    "        else:\n",
    "            data_mat = h5py.File(f\"{path}/real_data/{dataset}.h5\", \"r\")\n",
    "\n",
    "        y = np.array(data_mat['Y'])\n",
    "        x = np.array(data_mat['X'])\n",
    "        print(f\">>>>dataset {dataset}\")\n",
    "\n",
    "        for run in [1]:\n",
    "            start = time()\n",
    "            #### Run scDeepCluster on the simulated data\n",
    "            x = np.ceil(x).astype(np.int)\n",
    "            optimizer1 = Adam(amsgrad=True)\n",
    "            optimizer2 = 'adadelta'\n",
    "\n",
    "            # preprocessing scRNA-seq read counts matrix\n",
    "            adata = sc.AnnData(x)\n",
    "            adata.obs['Group'] = y\n",
    "\n",
    "            adata = read_dataset(adata,\n",
    "                                 transpose=False,\n",
    "                                 test_split=False,\n",
    "                                 copy=True)\n",
    "\n",
    "            adata = normalize(adata,\n",
    "                              size_factors=True,\n",
    "                              normalize_input=True,\n",
    "                              logtrans_input=True)\n",
    "\n",
    "            input_size = adata.n_vars\n",
    "\n",
    "            print('Sample size')\n",
    "            print(adata.X.shape)\n",
    "            print(y.shape)\n",
    "\n",
    "            x_sd = adata.X.std(0)\n",
    "            x_sd_median = np.median(x_sd)\n",
    "\n",
    "            update_interval = int(adata.X.shape[0] / 256)\n",
    "\n",
    "            seed = run\n",
    "            np.random.seed(seed)\n",
    "            # Define scDeepCluster model\n",
    "            scDeepCluster = SCDeepCluster(dims=[input_size, 256, 64, 32],\n",
    "                                          n_clusters=np.unique(y).shape[0],\n",
    "                                          noise_sd=2.5)\n",
    "\n",
    "            t0 = time()\n",
    "\n",
    "\n",
    "            # Pretrain autoencoders before clustering\n",
    "            scDeepCluster.pretrain(x=[adata.X, adata.obs.size_factors],\n",
    "                                   y=adata.raw.X,\n",
    "                                   batch_size=256,\n",
    "                                   epochs=600,\n",
    "                                   optimizer=optimizer1,\n",
    "                                   ae_file='ae_weights.h5')\n",
    "\n",
    "            # begin clustering, time not include pretraining part.\n",
    "\n",
    "            gamma = 1.  # set hyperparameter gamma\n",
    "            scDeepCluster.fit(\n",
    "                x_counts=adata.X,\n",
    "                sf=adata.obs.size_factors,\n",
    "                y=y,\n",
    "                raw_counts=adata.raw.X,\n",
    "                batch_size=256,\n",
    "                tol=0.001,\n",
    "                maxiter=20000,\n",
    "                update_interval=update_interval,\n",
    "                ae_weights=None,\n",
    "                save_dir='scDeepCluster',\n",
    "                loss_weights=[gamma, 1],\n",
    "                optimizer=optimizer2)\n",
    "\n",
    "            # Show the final results\n",
    "            y_pred = scDeepCluster.y_pred\n",
    "            nmi = np.round(\n",
    "                metrics.normalized_mutual_info_score(y, scDeepCluster.y_pred),\n",
    "                5)\n",
    "            ari = np.round(\n",
    "                metrics.adjusted_rand_score(y, scDeepCluster.y_pred), 5)\n",
    "            print('Final: NMI= %.4f, ARI= %.4f' % (nmi, ari))\n",
    "\n",
    "            elapsed = time() - start\n",
    "            ss = silhouette_score(scDeepCluster.features, scDeepCluster.y_pred)\n",
    "            cal = calinski_harabasz_score(scDeepCluster.features, scDeepCluster.y_pred)\n",
    "            \n",
    "            print(ss, cal)\n",
    "            df.loc[df.shape[0]] = [\n",
    "                dataset, ari, nmi, ss, run, elapsed, scDeepCluster.y_pred, cal, scDeepCluster.features\n",
    "            ]\n",
    "\n",
    "            df.to_pickle(\n",
    "                f\"../output/pickle_results/{category}/{category}_scDeepCluster.pkl\"\n",
    "            )\n",
    "            display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"dataset\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
